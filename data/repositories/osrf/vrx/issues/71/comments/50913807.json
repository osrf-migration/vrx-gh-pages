{"links": {"self": {"href": "data/repositories/osrf/vrx/issues/71/comments/50913807.json"}, "html": {"href": "#!/osrf/vrx/issues/71#comment-50913807"}}, "issue": {"links": {"self": {"href": "data/repositories/osrf/vrx/issues/71.json"}}, "type": "issue", "id": 71, "repository": {"links": {"self": {"href": "data/repositories/osrf/vrx.json"}, "html": {"href": "#!/osrf/vrx"}, "avatar": {"href": "data/bytebucket.org/ravatar/{e278ca38-7edb-4e62-b785-46dff5617d98}ts=2274605"}}, "type": "repository", "name": "vrx", "full_name": "osrf/vrx", "uuid": "{e278ca38-7edb-4e62-b785-46dff5617d98}"}, "title": "Workflow for docker"}, "content": {"raw": "I went back to look at this and realized that the tutorial in question is only intended to cover the case where someone wants to build the image using the latest source code (I had been thinking we also needed to cover the run-only case). Anyway, this means we have to download and compile our repository at some point. The problem is just that under option 2 we're doing it twice: once when we pull the build and run scripts, and once when we actually build the image. In my opinion, this needs to change anyway because re-running the build script will only rebuild the code if something in the Dockerfile changes (since that is all Docker is able to detect). This means that if people don't manually wipe out their stale layers (or delete the old images) the image won't update, which defeats the purpose of this use-case. It seems like too much to expect people to do this, and even if they do they will continue to have to build the last several layers from scratch, which is way worse than the one-time cost of downloading the repo on the host.\n\nWhat should happen instead, I think, is a modified version of 2 (above) which eliminates the duplication by making some changes to the Dockerfile. The workflow is roughly like this:\n\n1. Download the whole repo once on the host.\n1. Run the build script.\n1. The Dockerfile is altered so instead of downloading the repository, it expects to pull in all the files it needs from the local file system (that is, it assumes it is sitting in the subdirectory where we keep it in the repo).\n1. The Dockerfile builds the code using the copied files, then deletes the copies before finishing the image.\n1. Finally, the run script mounts the local repository into the image at runtime.\n\nThis might seem a bit more complicated but I think most of the complexity is hidden from the user and it gets the behavior we want. That is, we build the image the first time using the latest version of the code, and any subsequent pulls or updates will be immediately available in the image. Furthermore, all the time-saving features of our build tools should work properly. If that sounds good I can make a branch so we can test it to make sure it behaves as I claim it will. :)", "markup": "markdown", "html": "<p>I went back to look at this and realized that the tutorial in question is only intended to cover the case where someone wants to build the image using the latest source code (I had been thinking we also needed to cover the run-only case). Anyway, this means we have to download and compile our repository at some point. The problem is just that under option 2 we're doing it twice: once when we pull the build and run scripts, and once when we actually build the image. In my opinion, this needs to change anyway because re-running the build script will only rebuild the code if something in the Dockerfile changes (since that is all Docker is able to detect). This means that if people don't manually wipe out their stale layers (or delete the old images) the image won't update, which defeats the purpose of this use-case. It seems like too much to expect people to do this, and even if they do they will continue to have to build the last several layers from scratch, which is way worse than the one-time cost of downloading the repo on the host.</p>\n<p>What should happen instead, I think, is a modified version of 2 (above) which eliminates the duplication by making some changes to the Dockerfile. The workflow is roughly like this:</p>\n<ol>\n<li>Download the whole repo once on the host.</li>\n<li>Run the build script.</li>\n<li>The Dockerfile is altered so instead of downloading the repository, it expects to pull in all the files it needs from the local file system (that is, it assumes it is sitting in the subdirectory where we keep it in the repo).</li>\n<li>The Dockerfile builds the code using the copied files, then deletes the copies before finishing the image.</li>\n<li>Finally, the run script mounts the local repository into the image at runtime.</li>\n</ol>\n<p>This might seem a bit more complicated but I think most of the complexity is hidden from the user and it gets the behavior we want. That is, we build the image the first time using the latest version of the code, and any subsequent pulls or updates will be immediately available in the image. Furthermore, all the time-saving features of our build tools should work properly. If that sounds good I can make a branch so we can test it to make sure it behaves as I claim it will. :)</p>", "type": "rendered"}, "created_on": "2019-03-06T05:20:00.930544+00:00", "user": {"display_name": "Michael McCarrin", "uuid": "{3c3f500d-bbf6-472c-b8f7-627e5eabc226}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7B3c3f500d-bbf6-472c-b8f7-627e5eabc226%7D"}, "html": {"href": "https://bitbucket.org/%7B3c3f500d-bbf6-472c-b8f7-627e5eabc226%7D/"}, "avatar": {"href": "data/secure.gravatar.com/avatar/a50abc93a3175f92c79e7521dfe089c0d=httpsavatar-management--avatars.us-west-2.prod.public.atl-paas.netinitialsMM-0.png"}}, "nickname": "m1chaelm", "type": "user", "account_id": "5b2a9176467c9a09caa71ab1"}, "updated_on": null, "type": "issue_comment", "id": 50913807}